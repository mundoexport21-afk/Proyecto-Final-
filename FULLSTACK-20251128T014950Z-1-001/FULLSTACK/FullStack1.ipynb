{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÜ Notebook Comparativo Integrador\n",
    "## Selecci√≥n del Mejor Modelo de Machine Learning\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objetivo\n",
    "\n",
    "**Comparar todos los modelos entrenados** y seleccionar el mejor para el despliegue en producci√≥n.\n",
    "\n",
    "## üìä Modelos Evaluados\n",
    "\n",
    "### Aprendizaje Supervisado - Regresi√≥n:\n",
    "1. **Regresi√≥n Lineal**: Predicci√≥n de Valor FOB\n",
    "\n",
    "### Aprendizaje Supervisado - Clasificaci√≥n:\n",
    "2. **Regresi√≥n Log√≠stica**: Clasificaci√≥n multiclase\n",
    "3. **K-Nearest Neighbors (KNN)**: Clasificaci√≥n por vecinos\n",
    "4. **Support Vector Machine (SVM)**: M√°rgenes √≥ptimos\n",
    "5. **√Årboles de Decisi√≥n**: Reglas jer√°rquicas\n",
    "6. **Naive Bayes**: Clasificaci√≥n probabil√≠stica\n",
    "\n",
    "### Aprendizaje No Supervisado:\n",
    "7. **K-Means**: Clustering y segmentaci√≥n\n",
    "\n",
    "## üìè Criterios de Evaluaci√≥n\n",
    "\n",
    "**Para Modelos de Regresi√≥n:**\n",
    "- R¬≤ Score\n",
    "- RMSE (Root Mean Squared Error)\n",
    "- MAE (Mean Absolute Error)\n",
    "- MAPE (Mean Absolute Percentage Error)\n",
    "\n",
    "**Para Modelos de Clasificaci√≥n:**\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-Score\n",
    "- Validaci√≥n Cruzada\n",
    "\n",
    "**Para Modelos de Clustering:**\n",
    "- Silhouette Score\n",
    "- Davies-Bouldin Index\n",
    "- Inercia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importaci√≥n de Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Librer√≠as importadas exitosamente\n"
     ]
    }
   ],
   "source": [
    "# Librer√≠as b√°sicas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Librer√≠as de ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                            r2_score, mean_squared_error, mean_absolute_error)\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga de Modelos Entrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ CARGANDO MODELOS DESDE NOTEBOOKS...\n",
      "================================================================================\n",
      "   ‚ö†Ô∏è Error al leer ..\\üìì NOTEBOOKS\\02_Regresi√≤n_Lineal.ipynb: invalid syntax (<string>, line 23)\n",
      "   ‚ö†Ô∏è Regresi√≥n Lineal: No se encontraron m√©tricas en 02_Regresi√≤n_Lineal.ipynb\n",
      "‚úÖ Librer√≠as importadas\n",
      "   ‚ö†Ô∏è Error al leer ..\\üìì NOTEBOOKS\\03_Regresi√≤n_Logistica.ipynb: name 'model_lr' is not defined\n",
      "   ‚ö†Ô∏è Regresi√≥n Log√≠stica: No se encontraron m√©tricas en 03_Regresi√≤n_Logistica.ipynb\n",
      "   ‚ö†Ô∏è Error al leer ..\\üìì NOTEBOOKS\\04_KNN.ipynb: invalid syntax (<string>, line 20)\n",
      "   ‚ö†Ô∏è KNN: No se encontraron m√©tricas en 04_KNN.ipynb\n",
      "   ‚ö†Ô∏è Error al leer ..\\üìì NOTEBOOKS\\05_SVM.ipynb: invalid syntax (<string>, line 20)\n",
      "   ‚ö†Ô∏è SVM: No se encontraron m√©tricas en 05_SVM.ipynb\n",
      "   ‚ö†Ô∏è Error al leer ..\\üìì NOTEBOOKS\\06_Arboles_Decision.ipynb: invalid syntax (<string>, line 20)\n",
      "   ‚ö†Ô∏è √Årboles de Decisi√≥n: No se encontraron m√©tricas en 06_Arboles_Decision.ipynb\n",
      "   ‚ö†Ô∏è Error al leer ..\\üìì NOTEBOOKS\\07_Naive_Bayes.ipynb: invalid syntax (<string>, line 20)\n",
      "   ‚ö†Ô∏è Naive Bayes: No se encontraron m√©tricas en 07_Naive_Bayes.ipynb\n",
      "   ‚ö†Ô∏è Error al leer ..\\üìì NOTEBOOKS\\08_KMeans.ipynb: invalid syntax (<string>, line 19)\n",
      "   ‚ö†Ô∏è K-Means: No se encontraron m√©tricas en 08_KMeans.ipynb\n",
      "\n",
      "üìä Total de modelos con m√©tricas extra√≠das: 0\n",
      "\n",
      "=== Comparativa de modelos: regresion ===\n",
      "- Regresi√≥n Lineal: No disponible\n",
      "\n",
      "=== Comparativa de modelos: clasificacion ===\n",
      "- Regresi√≥n Log√≠stica: No disponible\n",
      "- KNN: No disponible\n",
      "- SVM: No disponible\n",
      "- √Årboles de Decisi√≥n: No disponible\n",
      "- Naive Bayes: No disponible\n",
      "\n",
      "=== Comparativa de modelos: clustering ===\n",
      "- K-Means: No disponible\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nbformat\n",
    "print(\"üì¶ CARGANDO MODELOS DESDE NOTEBOOKS...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "models = {}\n",
    "notebooks_dir = os.path.join('..', 'üìì NOTEBOOKS')\n",
    "notebook_files = [\n",
    "    ('Regresi√≥n Lineal', '02_Regresi√≤n_Lineal.ipynb', 'regresion'),\n",
    "    ('Regresi√≥n Log√≠stica', '03_Regresi√≤n_Logistica.ipynb', 'clasificacion'),\n",
    "    ('KNN', '04_KNN.ipynb', 'clasificacion'),\n",
    "    ('SVM', '05_SVM.ipynb', 'clasificacion'),\n",
    "    ('√Årboles de Decisi√≥n', '06_Arboles_Decision.ipynb', 'clasificacion'),\n",
    "    ('Naive Bayes', '07_Naive_Bayes.ipynb', 'clasificacion'),\n",
    "    ('K-Means', '08_KMeans.ipynb', 'clustering')\n",
    "]\n",
    "\n",
    "def extraer_modelo_de_notebook(notebook_path):\n",
    "    try:\n",
    "        nb = nbformat.read(open(notebook_path, encoding='utf-8'), as_version=4)\n",
    "        for cell in nb.cells:\n",
    "            if cell.cell_type == 'code' and 'metrics' in cell.source:\n",
    "                local_vars = {}\n",
    "                exec(cell.source, {}, local_vars)\n",
    "                if 'metrics' in local_vars:\n",
    "                    return {'metrics': local_vars['metrics']}\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error al leer {notebook_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "for name, filename, model_type in notebook_files:\n",
    "    notebook_path = os.path.join(notebooks_dir, filename)\n",
    "    if os.path.exists(notebook_path):\n",
    "        modelo = extraer_modelo_de_notebook(notebook_path)\n",
    "        if modelo:\n",
    "            models[name] = {'package': modelo, 'type': model_type}\n",
    "            print(f\"   ‚úì {name}: M√©tricas extra√≠das de {filename}\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è {name}: No se encontraron m√©tricas en {filename}\")\n",
    "            models[name] = {'package': None, 'type': model_type}\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è {name}: Notebook no encontrado - {filename}\")\n",
    "        models[name] = {'package': None, 'type': model_type}\n",
    "\n",
    "print(f\"\\nüìä Total de modelos con m√©tricas extra√≠das: {sum(1 for m in models.values() if m['package'] is not None)}\")\n",
    "\n",
    "def comparar_modelos_por_tipo(models, tipo):\n",
    "    print(f'\\n=== Comparativa de modelos: {tipo} ===')\n",
    "    for nombre, info in models.items():\n",
    "        if info['type'] == tipo and info['package'] is not None:\n",
    "            print(f'- {nombre}: M√©tricas disponibles')\n",
    "        elif info['type'] == tipo:\n",
    "            print(f'- {nombre}: No disponible')\n",
    "\n",
    "comparar_modelos_por_tipo(models, 'regresion')\n",
    "comparar_modelos_por_tipo(models, 'clasificacion')\n",
    "comparar_modelos_por_tipo(models, 'clustering')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extracci√≥n de M√©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä EXTRAYENDO M√âTRICAS DE LOS MODELOS...\n",
      "================================================================================\n",
      "\n",
      "üìã M√âTRICAS DE MODELOS DE CLASIFICACI√ìN:\n",
      "================================================================================\n",
      "   ‚ö†Ô∏è No hay modelos de clasificaci√≥n cargados\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä EXTRAYENDO M√âTRICAS DE LOS MODELOS...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# DataFrame para modelos de clasificaci√≥n\n",
    "classification_metrics = []\n",
    "\n",
    "for name, model_info in models.items():\n",
    "    if model_info['type'] == 'clasificacion' and model_info['package'] is not None:\n",
    "        metrics = model_info['package'].get('metrics', {})\n",
    "        classification_metrics.append({\n",
    "            'Modelo': name,\n",
    "            'Accuracy_Train': metrics.get('accuracy_train', np.nan),\n",
    "            'Accuracy_Test': metrics.get('accuracy_test', np.nan),\n",
    "            'Precision': metrics.get('precision', np.nan),\n",
    "            'Recall': metrics.get('recall', np.nan),\n",
    "            'F1_Score': metrics.get('f1_score', np.nan),\n",
    "            'CV_Accuracy': metrics.get('cv_accuracy_mean', np.nan)\n",
    "        })\n",
    "\n",
    "df_classification = pd.DataFrame(classification_metrics)\n",
    "\n",
    "print(\"\\nüìã M√âTRICAS DE MODELOS DE CLASIFICACI√ìN:\")\n",
    "print(\"=\"*80)\n",
    "if len(df_classification) > 0:\n",
    "    display(df_classification.style.format({\n",
    "        'Accuracy_Train': '{:.4f}',\n",
    "        'Accuracy_Test': '{:.4f}',\n",
    "        'Precision': '{:.4f}',\n",
    "        'Recall': '{:.4f}',\n",
    "        'F1_Score': '{:.4f}',\n",
    "        'CV_Accuracy': '{:.4f}'\n",
    "    }).background_gradient(cmap='RdYlGn', subset=['Accuracy_Test', 'F1_Score']))\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è No hay modelos de clasificaci√≥n cargados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã M√âTRICAS DE MODELOS DE REGRESI√ìN:\n",
      "================================================================================\n",
      "   ‚ö†Ô∏è No hay modelos de regresi√≥n cargados\n"
     ]
    }
   ],
   "source": [
    "# M√©tricas de regresi√≥n\n",
    "regression_metrics = []\n",
    "\n",
    "for name, model_info in models.items():\n",
    "    if model_info['type'] == 'regresion' and model_info['package'] is not None:\n",
    "        metrics = model_info['package'].get('metrics', {})\n",
    "        regression_metrics.append({\n",
    "            'Modelo': name,\n",
    "            'R2_Train': metrics.get('r2_train', np.nan),\n",
    "            'R2_Test': metrics.get('r2_test', np.nan),\n",
    "            'RMSE_Train': metrics.get('rmse_train', np.nan),\n",
    "            'RMSE_Test': metrics.get('rmse_test', np.nan),\n",
    "            'MAE_Test': metrics.get('mae_test', np.nan),\n",
    "            'CV_R2': metrics.get('cv_r2_mean', np.nan)\n",
    "        })\n",
    "\n",
    "df_regression = pd.DataFrame(regression_metrics)\n",
    "\n",
    "print(\"\\nüìã M√âTRICAS DE MODELOS DE REGRESI√ìN:\")\n",
    "print(\"=\"*80)\n",
    "if len(df_regression) > 0:\n",
    "    display(df_regression.style.format({\n",
    "        'R2_Train': '{:.4f}',\n",
    "        'R2_Test': '{:.4f}',\n",
    "        'RMSE_Train': '${:,.2f}',\n",
    "        'RMSE_Test': '${:,.2f}',\n",
    "        'MAE_Test': '${:,.2f}',\n",
    "        'CV_R2': '{:.4f}'\n",
    "    }).background_gradient(cmap='RdYlGn', subset=['R2_Test']))\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è No hay modelos de regresi√≥n cargados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã M√âTRICAS DE MODELOS DE CLUSTERING:\n",
      "================================================================================\n",
      "   ‚ö†Ô∏è No hay modelos de clustering cargados\n"
     ]
    }
   ],
   "source": [
    "# M√©tricas de clustering\n",
    "clustering_metrics = []\n",
    "\n",
    "for name, model_info in models.items():\n",
    "    if model_info['type'] == 'clustering' and model_info['package'] is not None:\n",
    "        metrics = model_info['package'].get('metrics', {})\n",
    "        clustering_metrics.append({\n",
    "            'Modelo': name,\n",
    "            'Silhouette': metrics.get('silhouette', np.nan),\n",
    "            'Davies_Bouldin': metrics.get('davies_bouldin', np.nan),\n",
    "            'Inercia': metrics.get('inertia', np.nan),\n",
    "            'N_Clusters': model_info['package'].get('n_clusters', np.nan)\n",
    "        })\n",
    "\n",
    "df_clustering = pd.DataFrame(clustering_metrics)\n",
    "\n",
    "print(\"\\nüìã M√âTRICAS DE MODELOS DE CLUSTERING:\")\n",
    "print(\"=\"*80)\n",
    "if len(df_clustering) > 0:\n",
    "    display(df_clustering.style.format({\n",
    "        'Silhouette': '{:.4f}',\n",
    "        'Davies_Bouldin': '{:.4f}',\n",
    "        'Inercia': '{:,.2f}',\n",
    "        'N_Clusters': '{:.0f}'\n",
    "    }))\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è No hay modelos de clustering cargados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizaciones Comparativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No hay suficientes modelos de clasificaci√≥n para comparar\n"
     ]
    }
   ],
   "source": [
    "# Gr√°fico comparativo de modelos de clasificaci√≥n\n",
    "if len(df_classification) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Accuracy\n",
    "    ax = axes[0, 0]\n",
    "    x = np.arange(len(df_classification))\n",
    "    width = 0.35\n",
    "    ax.bar(x - width/2, df_classification['Accuracy_Train'], width, label='Train', alpha=0.8)\n",
    "    ax.bar(x + width/2, df_classification['Accuracy_Test'], width, label='Test', alpha=0.8)\n",
    "    ax.set_ylabel('Accuracy', fontsize=11)\n",
    "    ax.set_title('Comparaci√≥n de Accuracy', fontsize=13, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df_classification['Modelo'], rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # F1-Score\n",
    "    ax = axes[0, 1]\n",
    "    ax.bar(df_classification['Modelo'], df_classification['F1_Score'], color='coral')\n",
    "    ax.set_ylabel('F1-Score', fontsize=11)\n",
    "    ax.set_title('Comparaci√≥n de F1-Score', fontsize=13, fontweight='bold')\n",
    "    ax.set_xticklabels(df_classification['Modelo'], rotation=45, ha='right')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Precision vs Recall\n",
    "    ax = axes[1, 0]\n",
    "    x = np.arange(len(df_classification))\n",
    "    ax.bar(x - width/2, df_classification['Precision'], width, label='Precision', alpha=0.8)\n",
    "    ax.bar(x + width/2, df_classification['Recall'], width, label='Recall', alpha=0.8)\n",
    "    ax.set_ylabel('Score', fontsize=11)\n",
    "    ax.set_title('Precision vs Recall', fontsize=13, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df_classification['Modelo'], rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Validaci√≥n Cruzada\n",
    "    ax = axes[1, 1]\n",
    "    ax.bar(df_classification['Modelo'], df_classification['CV_Accuracy'], color='lightgreen')\n",
    "    ax.set_ylabel('CV Accuracy', fontsize=11)\n",
    "    ax.set_title('Validaci√≥n Cruzada (5-Fold)', fontsize=13, fontweight='bold')\n",
    "    ax.set_xticklabels(df_classification['Modelo'], rotation=45, ha='right')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìä Gr√°ficos comparativos generados para modelos de clasificaci√≥n\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No hay suficientes modelos de clasificaci√≥n para comparar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No hay suficientes modelos de clasificaci√≥n para radar chart\n"
     ]
    }
   ],
   "source": [
    "# Radar chart para comparaci√≥n integral\n",
    "if len(df_classification) > 0:\n",
    "    from math import pi\n",
    "    \n",
    "    # Preparar datos para radar chart\n",
    "    categories = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "    N = len(categories)\n",
    "    \n",
    "    # Crear figura\n",
    "    fig = plt.figure(figsize=(14, 10))\n",
    "    ax = fig.add_subplot(111, projection='polar')\n",
    "    \n",
    "    # √Ångulos para cada m√©trica\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    # Colores para cada modelo\n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, len(df_classification)))\n",
    "    \n",
    "    # Plotear cada modelo\n",
    "    for idx, row in df_classification.iterrows():\n",
    "        values = [\n",
    "            row['Accuracy_Test'],\n",
    "            row['Precision'],\n",
    "            row['Recall'],\n",
    "            row['F1_Score']\n",
    "        ]\n",
    "        values += values[:1]\n",
    "        \n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=row['Modelo'], color=colors[idx])\n",
    "        ax.fill(angles, values, alpha=0.15, color=colors[idx])\n",
    "    \n",
    "    # Configuraci√≥n\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories, size=12)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('Comparaci√≥n Integral de Modelos de Clasificaci√≥n\\n(Radar Chart)', \n",
    "                size=16, fontweight='bold', pad=20)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìä Radar Chart generado exitosamente\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No hay suficientes modelos de clasificaci√≥n para radar chart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ranking y Selecci√≥n del Mejor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ RANKING DE MODELOS\n",
      "================================================================================\n",
      "\n",
      "‚ö†Ô∏è No hay modelos de clasificaci√≥n para rankear\n"
     ]
    }
   ],
   "source": [
    "print(\"üèÜ RANKING DE MODELOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ranking para modelos de clasificaci√≥n\n",
    "if len(df_classification) > 0:\n",
    "    print(\"\\nüìä MODELOS DE CLASIFICACI√ìN:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Calcular score compuesto (promedio ponderado)\n",
    "    df_classification['Score_Compuesto'] = (\n",
    "        0.30 * df_classification['Accuracy_Test'] +\n",
    "        0.25 * df_classification['F1_Score'] +\n",
    "        0.20 * df_classification['Precision'] +\n",
    "        0.15 * df_classification['Recall'] +\n",
    "        0.10 * df_classification['CV_Accuracy']\n",
    "    )\n",
    "    \n",
    "    # Ordenar por score compuesto\n",
    "    df_classification_ranked = df_classification.sort_values('Score_Compuesto', ascending=False)\n",
    "    df_classification_ranked['Rank'] = range(1, len(df_classification_ranked) + 1)\n",
    "    \n",
    "    display(df_classification_ranked[['Rank', 'Modelo', 'Accuracy_Test', 'F1_Score', \n",
    "                                      'Precision', 'Recall', 'Score_Compuesto']].style.format({\n",
    "        'Accuracy_Test': '{:.4f}',\n",
    "        'F1_Score': '{:.4f}',\n",
    "        'Precision': '{:.4f}',\n",
    "        'Recall': '{:.4f}',\n",
    "        'Score_Compuesto': '{:.4f}'\n",
    "    }).background_gradient(cmap='RdYlGn', subset=['Score_Compuesto']))\n",
    "    \n",
    "    # Mejor modelo\n",
    "    best_classification = df_classification_ranked.iloc[0]\n",
    "    print(f\"\\nü•á MEJOR MODELO DE CLASIFICACI√ìN: {best_classification['Modelo']}\")\n",
    "    print(f\"   ‚Ä¢ Accuracy Test: {best_classification['Accuracy_Test']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ F1-Score: {best_classification['F1_Score']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Score Compuesto: {best_classification['Score_Compuesto']:.4f}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No hay modelos de clasificaci√≥n para rankear\")\n",
    "    best_classification = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è No hay modelos de regresi√≥n para rankear\n"
     ]
    }
   ],
   "source": [
    "# Ranking para modelos de regresi√≥n\n",
    "if len(df_regression) > 0:\n",
    "    print(\"\\nüìä MODELOS DE REGRESI√ìN:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Para regresi√≥n, R¬≤ m√°s alto es mejor\n",
    "    df_regression_ranked = df_regression.sort_values('R2_Test', ascending=False)\n",
    "    df_regression_ranked['Rank'] = range(1, len(df_regression_ranked) + 1)\n",
    "    \n",
    "    display(df_regression_ranked[['Rank', 'Modelo', 'R2_Test', 'RMSE_Test', 'MAE_Test']].style.format({\n",
    "        'R2_Test': '{:.4f}',\n",
    "        'RMSE_Test': '${:,.2f}',\n",
    "        'MAE_Test': '${:,.2f}'\n",
    "    }).background_gradient(cmap='RdYlGn', subset=['R2_Test']))\n",
    "    \n",
    "    # Mejor modelo\n",
    "    best_regression = df_regression_ranked.iloc[0]\n",
    "    print(f\"\\nü•á MEJOR MODELO DE REGRESI√ìN: {best_regression['Modelo']}\")\n",
    "    print(f\"   ‚Ä¢ R¬≤ Test: {best_regression['R2_Test']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ RMSE Test: ${best_regression['RMSE_Test']:,.2f}\")\n",
    "    print(f\"   ‚Ä¢ MAE Test: ${best_regression['MAE_Test']:,.2f}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No hay modelos de regresi√≥n para rankear\")\n",
    "    best_regression = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è No hay modelos de clustering disponibles\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Informaci√≥n de clustering\n",
    "if len(df_clustering) > 0:\n",
    "    print(\"\\nüìä MODELOS DE CLUSTERING:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    display(df_clustering.style.format({\n",
    "        'Silhouette': '{:.4f}',\n",
    "        'Davies_Bouldin': '{:.4f}',\n",
    "        'Inercia': '{:,.2f}',\n",
    "        'N_Clusters': '{:.0f}'\n",
    "    }))\n",
    "    \n",
    "    best_clustering = df_clustering.iloc[0]\n",
    "    print(f\"\\nüìç MODELO DE CLUSTERING: {best_clustering['Modelo']}\")\n",
    "    print(f\"   ‚Ä¢ Silhouette Score: {best_clustering['Silhouette']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Davies-Bouldin Index: {best_clustering['Davies_Bouldin']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ N√∫mero de Clusters: {best_clustering['N_Clusters']:.0f}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No hay modelos de clustering disponibles\")\n",
    "    best_clustering = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. An√°lisis de Fortalezas y Debilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä AN√ÅLISIS COMPARATIVO DE MODELOS\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä AN√ÅLISIS COMPARATIVO DE MODELOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(df_classification) > 0:\n",
    "    print(\"\\nüîç AN√ÅLISIS DE CLASIFICACI√ìN:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Modelo con mejor accuracy\n",
    "    best_acc = df_classification.loc[df_classification['Accuracy_Test'].idxmax()]\n",
    "    print(f\"\\n‚ú® Mejor Accuracy: {best_acc['Modelo']} ({best_acc['Accuracy_Test']:.4f})\")\n",
    "    \n",
    "    # Modelo con mejor F1\n",
    "    best_f1 = df_classification.loc[df_classification['F1_Score'].idxmax()]\n",
    "    print(f\"‚ú® Mejor F1-Score: {best_f1['Modelo']} ({best_f1['F1_Score']:.4f})\")\n",
    "    \n",
    "    # Modelo con mejor balance precision-recall\n",
    "    df_classification['Balance_PR'] = df_classification['Precision'] * df_classification['Recall']\n",
    "    best_balance = df_classification.loc[df_classification['Balance_PR'].idxmax()]\n",
    "    print(f\"‚ú® Mejor Balance Precision-Recall: {best_balance['Modelo']}\")\n",
    "    \n",
    "    # Modelo m√°s estable (menor diferencia train-test)\n",
    "    df_classification['Estabilidad'] = abs(df_classification['Accuracy_Train'] - df_classification['Accuracy_Test'])\n",
    "    most_stable = df_classification.loc[df_classification['Estabilidad'].idxmin()]\n",
    "    print(f\"‚ú® Modelo M√°s Estable: {most_stable['Modelo']} (diferencia: {most_stable['Estabilidad']:.4f})\")\n",
    "    \n",
    "    # An√°lisis de overfitting\n",
    "    print(\"\\n‚ö†Ô∏è DETECCI√ìN DE OVERFITTING:\")\n",
    "    for idx, row in df_classification.iterrows():\n",
    "        diff = row['Accuracy_Train'] - row['Accuracy_Test']\n",
    "        if diff > 0.1:\n",
    "            print(f\"   ‚Ä¢ {row['Modelo']}: Posible overfitting (diferencia: {diff:.4f})\")\n",
    "        elif diff < -0.05:\n",
    "            print(f\"   ‚Ä¢ {row['Modelo']}: Comportamiento inusual (test > train)\")\n",
    "        else:\n",
    "            print(f\"   ‚Ä¢ {row['Modelo']}: Buen balance (diferencia: {diff:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Recomendaci√≥n Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üèÜ RECOMENDACI√ìN FINAL - MEJOR MODELO PARA PRODUCCI√ìN\n",
      "================================================================================\n",
      "\n",
      "‚ö†Ô∏è No hay modelos suficientes para hacer una recomendaci√≥n\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üèÜ RECOMENDACI√ìN FINAL - MEJOR MODELO PARA PRODUCCI√ìN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if best_classification is not None:\n",
    "    print(f\"\\nüéØ MODELO SELECCIONADO: {best_classification['Modelo']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    print(f\"\\nüìä M√âTRICAS DE RENDIMIENTO:\")\n",
    "    print(f\"   ‚Ä¢ Accuracy (Test): {best_classification['Accuracy_Test']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ F1-Score: {best_classification['F1_Score']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Precision: {best_classification['Precision']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Recall: {best_classification['Recall']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ CV Accuracy: {best_classification['CV_Accuracy']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Score Compuesto: {best_classification['Score_Compuesto']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ FORTALEZAS:\")\n",
    "    if best_classification['Accuracy_Test'] > 0.8:\n",
    "        print(f\"   ‚Ä¢ Excelente precisi√≥n general (Accuracy > 0.8)\")\n",
    "    if best_classification['F1_Score'] > 0.75:\n",
    "        print(f\"   ‚Ä¢ Buen balance entre Precision y Recall\")\n",
    "    if abs(best_classification['Accuracy_Train'] - best_classification['Accuracy_Test']) < 0.05:\n",
    "        print(f\"   ‚Ä¢ Modelo estable sin overfitting significativo\")\n",
    "    \n",
    "    print(f\"\\nüí° RAZONES DE SELECCI√ìN:\")\n",
    "    print(f\"   1. Mayor score compuesto entre todos los modelos\")\n",
    "    print(f\"   2. Balance √≥ptimo entre todas las m√©tricas\")\n",
    "    print(f\"   3. Validaci√≥n cruzada consistente\")\n",
    "    print(f\"   4. Adecuado para despliegue en producci√≥n\")\n",
    "    \n",
    "    print(f\"\\nüöÄ SIGUIENTE PASO:\")\n",
    "    print(f\"   Implementar {best_classification['Modelo']} en aplicaci√≥n FullStack\")\n",
    "\n",
    "elif best_regression is not None:\n",
    "    print(f\"\\nüéØ MODELO SELECCIONADO: {best_regression['Modelo']}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"\\nüìä M√âTRICAS DE RENDIMIENTO:\")\n",
    "    print(f\"   ‚Ä¢ R¬≤ (Test): {best_regression['R2_Test']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ RMSE (Test): ${best_regression['RMSE_Test']:,.2f}\")\n",
    "    print(f\"   ‚Ä¢ MAE (Test): ${best_regression['MAE_Test']:,.2f}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No hay modelos suficientes para hacer una recomendaci√≥n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Guardar Resultados de la Comparaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ GUARDANDO RESULTADOS...\n",
      "================================================================================\n",
      "   ‚úì best_model_selection.pkl\n",
      "\n",
      "‚úÖ Todos los resultados guardados exitosamente\n"
     ]
    }
   ],
   "source": [
    "# Guardar resultados\n",
    "print(\"üíæ GUARDANDO RESULTADOS...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(df_classification) > 0:\n",
    "    df_classification_ranked.to_csv('model_comparison_classification.csv', index=False)\n",
    "    print(\"   ‚úì model_comparison_classification.csv\")\n",
    "\n",
    "if len(df_regression) > 0:\n",
    "    df_regression_ranked.to_csv('model_comparison_regression.csv', index=False)\n",
    "    print(\"   ‚úì model_comparison_regression.csv\")\n",
    "\n",
    "if len(df_clustering) > 0:\n",
    "    df_clustering.to_csv('model_comparison_clustering.csv', index=False)\n",
    "    print(\"   ‚úì model_comparison_clustering.csv\")\n",
    "\n",
    "# Guardar informaci√≥n del mejor modelo\n",
    "best_model_info = {\n",
    "    'classification': best_classification.to_dict() if best_classification is not None else None,\n",
    "    'regression': best_regression.to_dict() if best_regression is not None else None,\n",
    "    'clustering': best_clustering.to_dict() if best_clustering is not None else None\n",
    "}\n",
    "\n",
    "with open('best_model_selection.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model_info, f)\n",
    "print(\"   ‚úì best_model_selection.pkl\")\n",
    "\n",
    "print(\"\\n‚úÖ Todos los resultados guardados exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Resumen Ejecutivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìã RESUMEN EJECUTIVO - COMPARACI√ìN DE MODELOS ML\n",
      "================================================================================\n",
      "\n",
      "üìä MODELOS EVALUADOS:\n",
      "   ‚Ä¢ Clasificaci√≥n: 0 modelos\n",
      "   ‚Ä¢ Regresi√≥n: 0 modelos\n",
      "   ‚Ä¢ Clustering: 0 modelos\n",
      "   ‚Ä¢ TOTAL: 0 modelos\n",
      "\n",
      "üìÅ ARCHIVOS GENERADOS:\n",
      "   ‚Ä¢ model_comparison_classification.csv\n",
      "   ‚Ä¢ model_comparison_regression.csv\n",
      "   ‚Ä¢ model_comparison_clustering.csv\n",
      "   ‚Ä¢ best_model_selection.pkl\n",
      "\n",
      "üöÄ PR√ìXIMO PASO:\n",
      "   Desarrollar aplicaci√≥n FullStack con el modelo seleccionado\n",
      "\n",
      "================================================================================\n",
      "‚úÖ AN√ÅLISIS COMPARATIVO COMPLETADO\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã RESUMEN EJECUTIVO - COMPARACI√ìN DE MODELOS ML\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä MODELOS EVALUADOS:\")\n",
    "print(f\"   ‚Ä¢ Clasificaci√≥n: {len(df_classification)} modelos\")\n",
    "print(f\"   ‚Ä¢ Regresi√≥n: {len(df_regression)} modelos\")\n",
    "print(f\"   ‚Ä¢ Clustering: {len(df_clustering)} modelos\")\n",
    "print(f\"   ‚Ä¢ TOTAL: {len(df_classification) + len(df_regression) + len(df_clustering)} modelos\")\n",
    "\n",
    "if best_classification is not None:\n",
    "    print(f\"\\nü•á GANADOR (Clasificaci√≥n): {best_classification['Modelo']}\")\n",
    "    print(f\"   ‚Ä¢ Accuracy: {best_classification['Accuracy_Test']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ F1-Score: {best_classification['F1_Score']:.4f}\")\n",
    "\n",
    "if best_regression is not None:\n",
    "    print(f\"\\nü•á GANADOR (Regresi√≥n): {best_regression['Modelo']}\")\n",
    "    print(f\"   ‚Ä¢ R¬≤: {best_regression['R2_Test']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ RMSE: ${best_regression['RMSE_Test']:,.2f}\")\n",
    "\n",
    "print(f\"\\nüìÅ ARCHIVOS GENERADOS:\")\n",
    "print(f\"   ‚Ä¢ model_comparison_classification.csv\")\n",
    "print(f\"   ‚Ä¢ model_comparison_regression.csv\")\n",
    "print(f\"   ‚Ä¢ model_comparison_clustering.csv\")\n",
    "print(f\"   ‚Ä¢ best_model_selection.pkl\")\n",
    "\n",
    "print(f\"\\nüöÄ PR√ìXIMO PASO:\")\n",
    "print(f\"   Desarrollar aplicaci√≥n FullStack con el modelo seleccionado\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ AN√ÅLISIS COMPARATIVO COMPLETADO\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
