{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîµ Modelo 8: K-Means Clustering\n",
    "## Segmentaci√≥n de Exportaciones (Aprendizaje No Supervisado)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objetivo\n",
    "Agrupar exportaciones colombianas en clusters similares usando el algoritmo K-Means, sin necesidad de etiquetas previas.\n",
    "\n",
    "K-Means es un algoritmo de clustering que particiona n observaciones en k clusters, donde cada observaci√≥n pertenece al cluster con la media m√°s cercana (centroide).\n",
    "\n",
    "## üìä Variables\n",
    "- **Features**: Variables num√©ricas relacionadas con las exportaciones\n",
    "- **M√©todo**: Clustering no supervisado\n",
    "- **Algoritmo**: K-Means con inicializaci√≥n inteligente\n",
    "\n",
    "## üìã Contenido\n",
    "1. Importaci√≥n de librer√≠as\n",
    "2. Carga y exploraci√≥n de datos\n",
    "3. Preprocesamiento de datos\n",
    "4. Determinaci√≥n del n√∫mero √≥ptimo de clusters\n",
    "5. Entrenamiento del modelo K-Means\n",
    "6. Evaluaci√≥n de clusters\n",
    "7. An√°lisis de caracter√≠sticas de clusters\n",
    "8. Visualizaci√≥n de resultados\n",
    "9. Guardado del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importaci√≥n de Librer√≠as\n",
    "\n",
    "Importamos todas las librer√≠as necesarias para el an√°lisis de clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librer√≠as necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Librer√≠as de machine learning\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "\n",
    "# Configuraci√≥n de gr√°ficos\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print('‚úÖ Librer√≠as importadas correctamente')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga de Datos\n",
    "\n",
    "Cargamos el dataset original desde el archivo Excel y realizamos una exploraci√≥n inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset desde Excel\n",
    "df = pd.read_excel('../DATA/DATAPROYECTO.xlsx', sheet_name='Detalle')\n",
    "\n",
    "print(f'üì¶ Dataset cargado exitosamente')\n",
    "print(f'   Dimensiones: {df.shape[0]:,} filas √ó {df.shape[1]} columnas')\n",
    "print(f'   Tama√±o en memoria: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB')\n",
    "\n",
    "# Vista r√°pida de las primeras filas\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocesamiento de Datos\n",
    "\n",
    "Realizamos el preprocesamiento necesario para preparar los datos para el clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una copia para trabajar\n",
    "df_processed = df.copy()\n",
    "\n",
    "print('üîß Iniciando preprocesamiento...')\n",
    "\n",
    "# 1. Manejo de valores faltantes\n",
    "print('\\n1Ô∏è‚É£ Tratamiento de valores faltantes:')\n",
    "\n",
    "# Para columnas num√©ricas: imputar con la mediana\n",
    "numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if df_processed[col].isnull().sum() > 0:\n",
    "        median_val = df_processed[col].median()\n",
    "        df_processed[col].fillna(median_val, inplace=True)\n",
    "        print(f'   ‚úì {col}: Imputado con mediana = {median_val:.2f}')\n",
    "\n",
    "print(f'\\n   Valores nulos restantes: {df_processed.isnull().sum().sum()}')\n",
    "\n",
    "# 2. Feature Engineering - Crear variables derivadas\n",
    "print('\\n2Ô∏è‚É£ Feature Engineering:')\n",
    "\n",
    "# Ratio Peso Bruto/Neto\n",
    "df_processed['Ratio_Peso_Bruto_Neto'] = df_processed['Peso en kilos brutos'] / (df_processed['Peso en kilos netos'] + 1e-10)\n",
    "print('   ‚úì Ratio_Peso_Bruto_Neto creado')\n",
    "\n",
    "# Valor por kilogramo\n",
    "df_processed['Valor_Por_Kg'] = df_processed['Valor FOB (USD)'] / (df_processed['Peso en kilos netos'] + 1e-10)\n",
    "print('   ‚úì Valor_Por_Kg creado')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Selecci√≥n de Features para Clustering\n",
    "\n",
    "Seleccionamos las variables num√©ricas m√°s relevantes para el clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar features num√©ricas para clustering\n",
    "features = [\n",
    "    'Valor FOB (USD)', 'Peso en kilos netos', 'Peso en kilos brutos',\n",
    "    'Cantidad(es)', 'N√∫mero de art√≠culos', 'Precio Unitario FOB (USD) Peso Neto',\n",
    "    'Ratio_Peso_Bruto_Neto', 'Valor_Por_Kg'\n",
    "]\n",
    "\n",
    "# Crear dataset para clustering\n",
    "df_clustering = df_processed[features].copy().dropna()\n",
    "df_clustering = df_clustering.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "# Tomar muestra para eficiencia si el dataset es muy grande\n",
    "sample_size = min(15000, len(df_clustering))  # M√°ximo 15k muestras para eficiencia\n",
    "if len(df_clustering) > sample_size:\n",
    "    df_clustering = df_clustering.sample(sample_size, random_state=42)\n",
    "    print(f'üìä Muestreo aplicado: {sample_size:,} registros de {len(df_processed):,}')\n",
    "\n",
    "print(f'üìä Dataset para clustering:')\n",
    "print(f'   ‚Ä¢ Features: {len(features)} variables num√©ricas')\n",
    "print(f'   ‚Ä¢ Muestras: {df_clustering.shape[0]:,}')\n",
    "\n",
    "# Estad√≠sticas descriptivas\n",
    "print('\\nüìà Estad√≠sticas descriptivas:')\n",
    "display(df_clustering.describe())\n",
    "\n",
    "# Escalamiento (crucial para K-Means)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_clustering)\n",
    "\n",
    "print(f'\\nüìè Escalamiento aplicado (StandardScaler)')\n",
    "print(f'   ‚Ä¢ Crucial para K-Means ya que usa distancias euclidianas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Determinaci√≥n del N√∫mero √ìptimo de Clusters\n",
    "\n",
    "Usamos el m√©todo del codo y otras t√©cnicas para determinar el n√∫mero √≥ptimo de clusters k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©todo del codo para encontrar k √≥ptimo\n",
    "print('üîç Determinando n√∫mero √≥ptimo de clusters...')\n",
    "\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    # Calcular silhouette score\n",
    "    if k > 1:  # Silhouette requiere al menos 2 clusters\n",
    "        sil_score = silhouette_score(X_scaled, clusters)\n",
    "        silhouette_scores.append(sil_score)\n",
    "\n",
    "# Visualizaci√≥n del m√©todo del codo\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Gr√°fico del codo\n",
    "ax1.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('N√∫mero de Clusters (k)', fontsize=12)\n",
    "ax1.set_ylabel('Inercia (Within-cluster sum of squares)', fontsize=12)\n",
    "ax1.set_title('M√©todo del Codo', fontweight='bold', fontsize=14)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Marcar el \"codo\" aproximado\n",
    "ax1.axvline(x=4, color='red', linestyle='--', alpha=0.7, label='k=4 sugerido')\n",
    "ax1.legend()\n",
    "\n",
    "# Gr√°fico de Silhouette Score\n",
    "ax2.plot(k_range[1:], silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('N√∫mero de Clusters (k)', fontsize=12)\n",
    "ax2.set_ylabel('Silhouette Score', fontsize=12)\n",
    "ax2.set_title('Silhouette Score', fontweight='bold', fontsize=14)\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.axvline(x=4, color='red', linestyle='--', alpha=0.7, label='k=4 sugerido')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('üí° Interpretaci√≥n:')\n",
    "print('   ‚Ä¢ M√©todo del codo: Buscar el \"codo\" donde la inercia deja de disminuir r√°pidamente')\n",
    "print('   ‚Ä¢ Silhouette Score: Valores m√°s cercanos a 1 indican mejores clusters')\n",
    "print(f'   ‚Ä¢ Mejor Silhouette Score: k={k_range[np.argmax(silhouette_scores)+1]} (score={max(silhouette_scores):.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entrenamiento del Modelo K-Means\n",
    "\n",
    "Entrenamos el modelo K-Means con el n√∫mero √≥ptimo de clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar K-Means con k=4 (basado en el an√°lisis de categor√≠as de valor)\n",
    "k_optimal = 4\n",
    "\n",
    "print(f'ü§ñ Entrenando K-Means con k={k_optimal}...')\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=k_optimal,\n",
    "    random_state=42,\n",
    "    n_init=10,  # N√∫mero de inicializaciones\n",
    "    init='k-means++',  # M√©todo de inicializaci√≥n inteligente\n",
    "    max_iter=300  # M√°ximo de iteraciones\n",
    ")\n",
    "\n",
    "# Entrenamiento\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Agregar clusters al dataframe\n",
    "df_clustering['Cluster'] = clusters\n",
    "\n",
    "print('‚úÖ K-Means entrenado exitosamente')\n",
    "print(f'   ‚Ä¢ N√∫mero de clusters: {k_optimal}')\n",
    "print(f'   ‚Ä¢ Algoritmo convergi√≥ en {kmeans.n_iter_} iteraciones')\n",
    "print(f'   ‚Ä¢ Inercia final: {kmeans.inertia_:.2f}')\n",
    "\n",
    "# Distribuci√≥n de clusters\n",
    "print(f'\\nüìä Distribuci√≥n de clusters:')\n",
    "cluster_counts = df_clustering['Cluster'].value_counts().sort_index()\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    percentage = (count / len(df_clustering)) * 100\n",
    "    print(f'   ‚Ä¢ Cluster {cluster_id}: {count:,} registros ({percentage:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluaci√≥n de la Calidad de los Clusters\n",
    "\n",
    "Evaluamos la calidad de los clusters usando m√©tricas espec√≠ficas para clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©tricas de evaluaci√≥n de clustering\n",
    "silhouette = silhouette_score(X_scaled, clusters)\n",
    "davies_bouldin = davies_bouldin_score(X_scaled, clusters)\n",
    "calinski_harabasz = calinski_harabasz_score(X_scaled, clusters)\n",
    "\n",
    "print('üìä M√âTRICAS DE EVALUACI√ìN DE CLUSTERING:')\n",
    "print('='*60)\n",
    "print(f'  Silhouette Score:     {silhouette:.4f}')\n",
    "print(f'    ‚Ä¢ Rango: [-1, 1] - M√°s cercano a 1 = mejor')\n",
    "print(f'    ‚Ä¢ > 0.5: Clusters razonablemente separados')\n",
    "print(f'    ‚Ä¢ > 0.7: Clusters bien separados')\n",
    "print()\n",
    "print(f'  Davies-Bouldin Index: {davies_bouldin:.4f}')\n",
    "print(f'    ‚Ä¢ Rango: [0, ‚àû] - M√°s cercano a 0 = mejor')\n",
    "print(f'    ‚Ä¢ < 1.0: Clusters aceptables')\n",
    "print()\n",
    "print(f'  Calinski-Harabasz:    {calinski_harabasz:.2f}')\n",
    "print(f'    ‚Ä¢ Rango: [0, ‚àû] - Valores m√°s altos = mejor')\n",
    "print(f'    ‚Ä¢ Mide la separaci√≥n entre clusters')\n",
    "print()\n",
    "print(f'  Inercia (Within-cluster sum of squares): {kmeans.inertia_:.2f}')\n",
    "print(f'    ‚Ä¢ Suma de distancias cuadradas dentro de clusters')\n",
    "print(f'    ‚Ä¢ Valores m√°s bajos = mejor compacidad')\n",
    "\n",
    "# Interpretaci√≥n autom√°tica\n",
    "print('\\nüéØ Interpretaci√≥n autom√°tica:')\n",
    "if silhouette > 0.5:\n",
    "    print('   ‚úÖ Clusters bien separados (Silhouette > 0.5)')\n",
    "elif silhouette > 0.25:\n",
    "    print('   ‚ö†Ô∏è Clusters razonables (Silhouette > 0.25)')\n",
    "else:\n",
    "    print('   ‚ùå Clusters pobremente separados (Silhouette < 0.25)')\n",
    "\n",
    "if davies_bouldin < 1.0:\n",
    "    print('   ‚úÖ Buena separaci√≥n entre clusters (DB < 1.0)')\n",
    "else:\n",
    "    print('   ‚ö†Ô∏è Separaci√≥n entre clusters podr√≠a mejorar (DB > 1.0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. An√°lisis de Caracter√≠sticas de los Clusters\n",
    "\n",
    "Analizamos las caracter√≠sticas principales de cada cluster para entender su significado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis detallado de cada cluster\n",
    "print('üìã AN√ÅLISIS DETALLADO DE CLUSTERS:')\n",
    "print('='*80)\n",
    "\n",
    "cluster_profiles = []\n",
    "\n",
    "for i in range(k_optimal):\n",
    "    cluster_data = df_clustering[df_clustering['Cluster'] == i]\n",
    "    \n",
    "    print(f'\\nüîµ CLUSTER {i} (n={len(cluster_data):,})')\n",
    "    print('-' * 50)\n",
    "    \n",
    "    # Estad√≠sticas clave\n",
    "    stats = cluster_data[features].describe()\n",
    "    \n",
    "    print(f'üìä Estad√≠sticas principales:')\n",
    "    print(f'   ‚Ä¢ Valor FOB: ${cluster_data[\"Valor FOB (USD)\"].mean():,.2f} ¬± ${cluster_data[\"Valor FOB (USD)\"].std():,.2f}')\n",
    "    print(f'   ‚Ä¢ Peso neto: {cluster_data[\"Peso en kilos netos\"].mean():,.2f} ¬± {cluster_data[\"Peso en kilos netos\"].std():,.2f} kg')\n",
    "    print(f'   ‚Ä¢ Cantidad: {cluster_data[\"Cantidad(es)\"].mean():,.2f} ¬± {cluster_data[\"Cantidad(es)\"].std():,.2f}')\n",
    "    print(f'   ‚Ä¢ Precio unitario: ${cluster_data[\"Precio Unitario FOB (USD) Peso Neto\"].mean():,.2f}')\n",
    "    \n",
    "    # Caracter√≠sticas distintivas (comparadas con la media global)\n",
    "    print(f'\\nüéØ Caracter√≠sticas distintivas:')\n",
    "    global_mean = df_clustering[features].mean()\n",
    "    cluster_mean = cluster_data[features].mean()\n",
    "    \n",
    "    for feature in features:\n",
    "        diff_pct = ((cluster_mean[feature] - global_mean[feature]) / global_mean[feature]) * 100\n",
    "        if abs(diff_pct) > 20:  # M√°s de 20% de diferencia\n",
    "            direction = \"mayor\" if diff_pct > 0 else \"menor\"\n",
    "            print(f'   ‚Ä¢ {feature}: {abs(diff_pct):.1f}% {direction} que el promedio')\n",
    "    \n",
    "    # Guardar perfil del cluster\n",
    "    cluster_profiles.append({\n",
    "        'cluster_id': i,\n",
    "        'size': len(cluster_data),\n",
    "        'percentage': (len(cluster_data) / len(df_clustering)) * 100,\n",
    "        'features_mean': cluster_mean.to_dict(),\n",
    "        'features_std': cluster_data[features].std().to_dict()\n",
    "    })\n",
    "\n",
    "# Resumen comparativo\n",
    "print(f'\\nüìä RESUMEN COMPARATIVO DE CLUSTERS:')\n",
    "print('='*80)\n",
    "summary_df = df_clustering.groupby('Cluster')[features[:3]].mean()  # Solo primeras 3 features para resumen\n",
    "display(summary_df.style.background_gradient(cmap='Blues', axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizaci√≥n de los Clusters\n",
    "\n",
    "Visualizamos los clusters usando reducci√≥n de dimensionalidad (PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducci√≥n de dimensionalidad con PCA para visualizaci√≥n\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Visualizaci√≥n 2D de los clusters\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Scatter plot de los puntos\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                     c=clusters, cmap='viridis', \n",
    "                     alpha=0.6, s=50, edgecolors='white', linewidth=0.5)\n",
    "\n",
    "# Centroides de los clusters\n",
    "centers_pca = pca.transform(kmeans.cluster_centers_)\n",
    "plt.scatter(centers_pca[:, 0], centers_pca[:, 1], \n",
    "           c='red', marker='X', s=400, edgecolors='black', linewidths=3,\n",
    "           label='Centroides')\n",
    "\n",
    "# Etiquetas y t√≠tulo\n",
    "plt.xlabel(f'Componente Principal 1 ({pca.explained_variance_ratio_[0]:.1%} varianza)', fontsize=12)\n",
    "plt.ylabel(f'Componente Principal 2 ({pca.explained_variance_ratio_[1]:.1%} varianza)', fontsize=12)\n",
    "plt.title('Visualizaci√≥n de Clusters K-Means (Reducci√≥n PCA)', \n",
    "          fontsize=16, fontweight='bold')\n",
    "\n",
    "# Barra de colores\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Cluster', fontsize=12)\n",
    "\n",
    "# Leyenda\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('üìä Informaci√≥n de la visualizaci√≥n:')\n",
    "print(f'   ‚Ä¢ Varianza explicada por las 2 componentes: {pca.explained_variance_ratio_.sum():.1%}')\n",
    "print('   ‚Ä¢ Cada punto representa una exportaci√≥n')\n",
    "print('   ‚Ä¢ Colores indican la pertenencia al cluster')\n",
    "print('   ‚Ä¢ X rojas marcan los centroides de cada cluster')\n",
    "\n",
    "# Informaci√≥n adicional sobre PCA\n",
    "print(f'\\nüîç Componentes principales:')\n",
    "for i, component in enumerate(pca.components_):\n",
    "    top_features = np.argsort(np.abs(component))[-3:]  # Top 3 features\n",
    "    print(f'   ‚Ä¢ PC{i+1}: {features[top_features[2]]}, {features[top_features[1]]}, {features[top_features[0]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Guardado del Modelo\n",
    "\n",
    "Guardamos el modelo de clustering junto con todos los objetos necesarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar paquete completo del modelo\n",
    "model_package = {\n",
    "    'model': kmeans,\n",
    "    'scaler': scaler,\n",
    "    'pca': pca,\n",
    "    'features': features,\n",
    "    'n_clusters': k_optimal,\n",
    "    'cluster_profiles': cluster_profiles,\n",
    "    'metrics': {\n",
    "        'silhouette_score': silhouette,\n",
    "        'davies_bouldin_index': davies_bouldin,\n",
    "        'calinski_harabasz_score': calinski_harabasz,\n",
    "        'inertia': kmeans.inertia_,\n",
    "        'n_iterations': kmeans.n_iter_\n",
    "    },\n",
    "    'model_info': {\n",
    "        'algorithm': 'K-Means Clustering',\n",
    "        'n_clusters': k_optimal,\n",
    "        'init_method': 'k-means++',\n",
    "        'n_init': 10,\n",
    "        'max_iter': 300,\n",
    "        'random_state': 42,\n",
    "        'n_features': len(features),\n",
    "        'n_samples': len(df_clustering),\n",
    "        'features_list': features\n",
    "    }\n",
    "}\n",
    "\n",
    "# Guardar el modelo\n",
    "with open('model_kmeans.pkl', 'wb') as f:\n",
    "    pickle.dump(model_package, f)\n",
    "\n",
    "print('üíæ Modelo guardado exitosamente')\n",
    "print('   ‚Ä¢ Archivo: model_kmeans.pkl')\n",
    "print('   ‚Ä¢ Contiene: modelo K-Means, scaler, PCA, perfiles de clusters y m√©tricas')\n",
    "\n",
    "# Verificar que se guard√≥ correctamente\n",
    "import os\n",
    "if os.path.exists('model_kmeans.pkl'):\n",
    "    size = os.path.getsize('model_kmeans.pkl') / 1024\n",
    "    print(f'   ‚Ä¢ Tama√±o: {size:.2f} KB')\n",
    "    print('‚úÖ Verificaci√≥n exitosa')\n",
    "else:\n",
    "    print('‚ùå Error al guardar el modelo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Conclusiones\n",
    "\n",
    "### Resumen del Clustering K-Means:\n",
    "\n",
    "**Fortalezas:**\n",
    "- **Simplicidad**: Algoritmo f√°cil de entender e implementar\n",
    "- **Escalabilidad**: Funciona bien con grandes datasets\n",
    "- **Interpretabilidad**: Los centroides son f√°ciles de analizar\n",
    "- **Velocidad**: R√°pido para datos num√©ricos\n",
    "- **Determin√≠stico**: Resultados reproducibles con semilla fija\n",
    "\n",
    "**Limitaciones:**\n",
    "- **N√∫mero de clusters**: Requiere definir k a priori\n",
    "- **Formas esf√©ricas**: Asume clusters de forma esf√©rica\n",
    "- **Escala sensible**: Requiere escalamiento de features\n",
    "- **Outliers**: Sensible a valores at√≠picos\n",
    "- **Inicializaci√≥n**: Puede converger a √≥ptimos locales\n",
    "\n",
    "### Resultados Obtenidos:\n",
    "- **N√∫mero de clusters:** 4\n",
    "- **Muestras clusterizadas:** {len(df_clustering):,}\n",
    "- **Silhouette Score:** {silhouette:.4f}\n",
    "- **Davies-Bouldin Index:** {davies_bouldin:.4f}\n",
    "- **Inercia final:** {kmeans.inertia_:.2f}\n",
    "- **Varianza explicada por PCA:** {pca.explained_variance_ratio_.sum():.1%}\n",
    "\n",
    "### Distribuci√≥n de Clusters:\n",
    "{cluster_counts.to_dict()}\n",
    "\n",
    "### Recomendaciones:\n",
    "1. **Interpretaci√≥n de clusters**: Analizar los perfiles de cada cluster para asignar nombres descriptivos\n",
    "2. **Validaci√≥n externa**: Si hay etiquetas disponibles, comparar con clustering supervisado\n",
    "3. **T√©cnicas alternativas**: Considerar DBSCAN o Gaussian Mixture Models para clusters no esf√©ricos\n",
    "4. **Reducci√≥n de features**: Si hay muchas features, considerar PCA antes del clustering\n",
    "5. **Robustez**: Ejecutar m√∫ltiples inicializaciones para asegurar estabilidad\n",
    "\n",
    "---\n",
    "\n",
    "**üîµ Modelo K-Means completado exitosamente** ‚úÖ\n",
    "\n",
    "*Los datos han sido segmentados en clusters naturales que pueden usarse para an√°lisis de patrones o estrategias de negocio.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}