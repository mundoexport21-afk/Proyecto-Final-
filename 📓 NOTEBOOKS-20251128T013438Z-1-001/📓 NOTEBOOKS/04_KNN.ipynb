{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Modelo 3: K-Nearest Neighbors (KNN)\n",
    "## Clasificaci√≥n por Vecinos M√°s Cercanos\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objetivo\n",
    "Clasificar exportaciones colombianas en categor√≠as de valor usando el algoritmo K-Nearest Neighbors (KNN).\n",
    "\n",
    "El KNN es un algoritmo de aprendizaje supervisado que clasifica nuevos ejemplos bas√°ndose en la similitud con los k ejemplos m√°s cercanos en el espacio de caracter√≠sticas.\n",
    "\n",
    "## üìä Variables\n",
    "- **Target (Variable objetivo)**: `Categoria_Valor` - Clasificaci√≥n del valor FOB en categor√≠as (Bajo/Medio/Alto/Muy Alto)\n",
    "- **Features (Caracter√≠sticas)**: Variables num√©ricas y categ√≥ricas codificadas relacionadas con las exportaciones\n",
    "\n",
    "## üìã Contenido\n",
    "1. Importaci√≥n de librer√≠as\n",
    "2. Carga y exploraci√≥n de datos\n",
    "3. Preprocesamiento de datos\n",
    "4. Preparaci√≥n de features y target\n",
    "5. Entrenamiento del modelo KNN\n",
    "6. Evaluaci√≥n del modelo\n",
    "7. Validaci√≥n cruzada\n",
    "8. Guardado del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importaci√≥n de Librer√≠as\n",
    "\n",
    "Importamos todas las librer√≠as necesarias para el an√°lisis y modelado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librer√≠as necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Librer√≠as de machine learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pickle\n",
    "\n",
    "# Configuraci√≥n de gr√°ficos\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print('‚úÖ Librer√≠as importadas correctamente')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga de Datos\n",
    "\n",
    "Cargamos el dataset original desde el archivo Excel y realizamos una exploraci√≥n inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset desde Excel\n",
    "df = pd.read_excel('../DATA/DATAPROYECTO.xlsx', sheet_name='Detalle')\n",
    "\n",
    "print(f'üì¶ Dataset cargado exitosamente')\n",
    "print(f'   Dimensiones: {df.shape[0]:,} filas √ó {df.shape[1]} columnas')\n",
    "print(f'   Tama√±o en memoria: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB')\n",
    "\n",
    "# Vista r√°pida de las primeras filas\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocesamiento de Datos\n",
    "\n",
    "Realizamos el preprocesamiento necesario para preparar los datos para el modelo KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una copia para trabajar\n",
    "df_processed = df.copy()\n",
    "\n",
    "print('üîß Iniciando preprocesamiento...')\n",
    "\n",
    "# 1. Manejo de valores faltantes\n",
    "print('\\n1Ô∏è‚É£ Tratamiento de valores faltantes:')\n",
    "\n",
    "# Para columnas num√©ricas: imputar con la mediana\n",
    "numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if df_processed[col].isnull().sum() > 0:\n",
    "        median_val = df_processed[col].median()\n",
    "        df_processed[col].fillna(median_val, inplace=True)\n",
    "        print(f'   ‚úì {col}: Imputado con mediana = {median_val:.2f}')\n",
    "\n",
    "# Para columnas categ√≥ricas: imputar con 'Desconocido'\n",
    "categorical_cols = df_processed.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if df_processed[col].isnull().sum() > 0:\n",
    "        df_processed[col].fillna('Desconocido', inplace=True)\n",
    "        print(f'   ‚úì {col}: Imputado con \"Desconocido\"')\n",
    "\n",
    "print(f'\\n   Valores nulos restantes: {df_processed.isnull().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Feature Engineering - Crear variables derivadas\n",
    "print('\\n2Ô∏è‚É£ Feature Engineering:')\n",
    "\n",
    "# Ratio Peso Bruto/Neto\n",
    "df_processed['Ratio_Peso_Bruto_Neto'] = df_processed['Peso en kilos brutos'] / (df_processed['Peso en kilos netos'] + 1e-10)\n",
    "print('   ‚úì Ratio_Peso_Bruto_Neto creado')\n",
    "\n",
    "# Valor por kilogramo\n",
    "df_processed['Valor_Por_Kg'] = df_processed['Valor FOB (USD)'] / (df_processed['Peso en kilos netos'] + 1e-10)\n",
    "print('   ‚úì Valor_Por_Kg creado')\n",
    "\n",
    "# Clasificaci√≥n de valor de exportaci√≥n (target)\n",
    "def clasificar_valor(valor):\n",
    "    if valor < 1000:\n",
    "        return 'Bajo'\n",
    "    elif valor < 10000:\n",
    "        return 'Medio'\n",
    "    elif valor < 100000:\n",
    "        return 'Alto'\n",
    "    else:\n",
    "        return 'Muy Alto'\n",
    "\n",
    "df_processed['Categoria_Valor'] = df_processed['Valor FOB (USD)'].apply(clasificar_valor)\n",
    "print('   ‚úì Categoria_Valor creado (Bajo/Medio/Alto/Muy Alto)')\n",
    "\n",
    "# Ver distribuci√≥n de la variable target\n",
    "print('\\nüìä Distribuci√≥n de la variable target:')\n",
    "print(df_processed['Categoria_Valor'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Codificaci√≥n de variables categ√≥ricas\n",
    "print('\\n3Ô∏è‚É£ Codificaci√≥n de variables categ√≥ricas:')\n",
    "\n",
    "# Variables categ√≥ricas a codificar\n",
    "categorical_to_encode = ['Pa√≠s de Destino', 'Continente Destino', 'Departamento Origen', 'V√≠a de transporte']\n",
    "\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_to_encode:\n",
    "    if col in df_processed.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_processed[col + '_encoded'] = le.fit_transform(df_processed[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f'   ‚úì {col}: Codificado ({df_processed[col].nunique()} categor√≠as ‚Üí {len(le.classes_)} clases)')\n",
    "\n",
    "print(f'\\n   Total de variables codificadas: {len(label_encoders)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparaci√≥n de Features y Target\n",
    "\n",
    "Seleccionamos las caracter√≠sticas (features) y la variable objetivo (target) para el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir target y features\n",
    "target = 'Categoria_Valor'\n",
    "\n",
    "# Seleccionar features num√©ricas y codificadas\n",
    "features = [\n",
    "    'Peso en kilos netos', 'Peso en kilos brutos', 'Cantidad(es)',\n",
    "    'N√∫mero de art√≠culos', 'Precio Unitario FOB (USD) Peso Neto',\n",
    "    'Pa√≠s de Destino_encoded', 'Continente Destino_encoded',\n",
    "    'Departamento Origen_encoded', 'V√≠a de transporte_encoded',\n",
    "    'Ratio_Peso_Bruto_Neto', 'Valor_Por_Kg'\n",
    "]\n",
    "\n",
    "# Crear dataset final\n",
    "df_model = df_processed[features + [target]].copy().dropna()\n",
    "df_model = df_model.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "# Codificar el target\n",
    "le_target = LabelEncoder()\n",
    "y = le_target.fit_transform(df_model[target])\n",
    "\n",
    "X = df_model[features]\n",
    "\n",
    "print(f'üìä Dataset preparado:')\n",
    "print(f'   ‚Ä¢ Features: {len(features)} variables')\n",
    "print(f'   ‚Ä¢ Muestras: {X.shape[0]:,}')\n",
    "print(f'   ‚Ä¢ Target: {len(np.unique(y))} clases ({le_target.classes_})')\n",
    "\n",
    "# Ver balance de clases\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x=le_target.inverse_transform(y))\n",
    "plt.title('Distribuci√≥n de Clases - Variable Target')\n",
    "plt.xlabel('Categor√≠a de Valor')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Divisi√≥n y Escalamiento de Datos\n",
    "\n",
    "Dividimos los datos en conjuntos de entrenamiento y prueba, y escalamos las caracter√≠sticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisi√≥n train/test con estratificaci√≥n\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f'‚úÇÔ∏è Divisi√≥n de datos:')\n",
    "print(f'   ‚Ä¢ Train: {X_train.shape[0]:,} muestras')\n",
    "print(f'   ‚Ä¢ Test: {X_test.shape[0]:,} muestras')\n",
    "\n",
    "# Escalamiento de features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f'üìè Escalamiento aplicado (StandardScaler)')\n",
    "print(f'   ‚Ä¢ Media ‚âà 0, Desviaci√≥n est√°ndar ‚âà 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entrenamiento del Modelo KNN\n",
    "\n",
    "Entrenamos el modelo K-Nearest Neighbors con los hiperpar√°metros seleccionados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ü§ñ Entrenando modelo K-Nearest Neighbors...')\n",
    "\n",
    "# Configuraci√≥n del modelo\n",
    "model = KNeighborsClassifier(\n",
    "    n_neighbors=5,           # N√∫mero de vecinos\n",
    "    weights='distance',      # Peso por distancia (m√°s cercano = m√°s peso)\n",
    "    metric='euclidean'       # Distancia euclidiana\n",
    ")\n",
    "\n",
    "# Entrenamiento\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print('‚úÖ Modelo entrenado exitosamente')\n",
    "print(f'   ‚Ä¢ Algoritmo: K-Nearest Neighbors')\n",
    "print(f'   ‚Ä¢ k: {model.n_neighbors}')\n",
    "print(f'   ‚Ä¢ Peso: {model.weights}')\n",
    "print(f'   ‚Ä¢ M√©trica: {model.metric}')\n",
    "\n",
    "# Predicciones\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print('\\nüîÆ Predicciones realizadas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluaci√≥n del Modelo\n",
    "\n",
    "Evaluamos el rendimiento del modelo usando varias m√©tricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√°lculo de m√©tricas\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "acc_test = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print('üìä M√âTRICAS DE EVALUACI√ìN:')\n",
    "print('='*50)\n",
    "print(f'  Accuracy Train: {acc_train:.4f}')\n",
    "print(f'  Accuracy Test:  {acc_test:.4f}')\n",
    "print(f'  Precision:      {precision:.4f}')\n",
    "print(f'  Recall:         {recall:.4f}')\n",
    "print(f'  F1-Score:       {f1:.4f}')\n",
    "\n",
    "# Verificar overfitting/underfitting\n",
    "if acc_train - acc_test > 0.1:\n",
    "    print('\\n‚ö†Ô∏è  Posible overfitting detectado')\n",
    "elif acc_test < 0.6:\n",
    "    print('\\n‚ö†Ô∏è  Posible underfitting detectado')\n",
    "else:\n",
    "    print('\\n‚úÖ Rendimiento equilibrado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reporte de clasificaci√≥n detallado\n",
    "print('\\nüìã REPORTE DE CLASIFICACI√ìN DETALLADO:')\n",
    "print('='*50)\n",
    "print(classification_report(y_test, y_test_pred, target_names=le_target.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Matriz de Confusi√≥n\n",
    "\n",
    "Visualizamos la matriz de confusi√≥n para entender mejor los errores de clasificaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusi√≥n\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=le_target.classes_, yticklabels=le_target.classes_)\n",
    "plt.title('Matriz de Confusi√≥n - Modelo KNN', fontweight='bold', fontsize=14)\n",
    "plt.xlabel('Predicci√≥n', fontsize=12)\n",
    "plt.ylabel('Real', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('üìä Interpretaci√≥n de la matriz de confusi√≥n:')\n",
    "print('   ‚Ä¢ Diagonal principal: Predicciones correctas')\n",
    "print('   ‚Ä¢ Fuera de diagonal: Errores de clasificaci√≥n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Validaci√≥n Cruzada\n",
    "\n",
    "Realizamos validaci√≥n cruzada para obtener una estimaci√≥n m√°s robusta del rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validaci√≥n cruzada\n",
    "cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print('üîÑ VALIDACI√ìN CRUZADA (5-fold):')\n",
    "print('='*50)\n",
    "print(f'  Scores individuales: {cv_scores}')\n",
    "print(f'  Accuracy promedio: {cv_scores.mean():.4f}')\n",
    "print(f'  Desviaci√≥n est√°ndar: {cv_scores.std():.4f}')\n",
    "print(f'  Rango: [{cv_scores.min():.4f}, {cv_scores.max():.4f}]')\n",
    "\n",
    "# Comparaci√≥n con accuracy de test\n",
    "print(f'\\nüìä Comparaci√≥n:')\n",
    "print(f'  CV Accuracy: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}')\n",
    "print(f'  Test Accuracy: {acc_test:.4f}')\n",
    "\n",
    "if abs(cv_scores.mean() - acc_test) < 0.05:\n",
    "    print('‚úÖ Resultados consistentes')\n",
    "else:\n",
    "    print('‚ö†Ô∏è  Diferencia significativa entre CV y test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Guardado del Modelo\n",
    "\n",
    "Guardamos el modelo entrenado junto con los objetos necesarios para su uso posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar paquete del modelo\n",
    "model_package = {\n",
    "    'model': model,\n",
    "    'scaler': scaler,\n",
    "    'label_encoder_target': le_target,\n",
    "    'label_encoders_features': label_encoders,\n",
    "    'features': features,\n",
    "    'target': target,\n",
    "    'metrics': {\n",
    "        'accuracy_train': acc_train,\n",
    "        'accuracy_test': acc_test,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'cv_accuracy_mean': cv_scores.mean(),\n",
    "        'cv_accuracy_std': cv_scores.std()\n",
    "    },\n",
    "    'model_info': {\n",
    "        'algorithm': 'K-Nearest Neighbors',\n",
    "        'n_neighbors': model.n_neighbors,\n",
    "        'weights': model.weights,\n",
    "        'metric': model.metric,\n",
    "        'n_features': len(features),\n",
    "        'n_classes': len(le_target.classes_),\n",
    "        'classes': list(le_target.classes_)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Guardar el modelo\n",
    "with open('model_knn.pkl', 'wb') as f:\n",
    "    pickle.dump(model_package, f)\n",
    "\n",
    "print('üíæ Modelo guardado exitosamente')\n",
    "print('   ‚Ä¢ Archivo: model_knn.pkl')\n",
    "print('   ‚Ä¢ Contiene: modelo, scaler, encoders y m√©tricas')\n",
    "\n",
    "# Verificar que se guard√≥ correctamente\n",
    "import os\n",
    "if os.path.exists('model_knn.pkl'):\n",
    "    size = os.path.getsize('model_knn.pkl') / 1024\n",
    "    print(f'   ‚Ä¢ Tama√±o: {size:.2f} KB')\n",
    "    print('‚úÖ Verificaci√≥n exitosa')\n",
    "else:\n",
    "    print('‚ùå Error al guardar el modelo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Conclusiones\n",
    "\n",
    "### Resumen del Modelo KNN:\n",
    "\n",
    "**Fortalezas:**\n",
    "- Algoritmo simple e interpretable\n",
    "- No requiere supuestos sobre la distribuci√≥n de los datos\n",
    "- Funciona bien con datasets peque√±os a medianos\n",
    "- Robusto a outliers cuando se usa distancia apropiada\n",
    "\n",
    "**Limitaciones:**\n",
    "- Sensible a la escala de las variables (requiere escalamiento)\n",
    "- Computacionalmente costoso en datasets grandes\n",
    "- Sensible al n√∫mero k y a la m√©trica de distancia\n",
    "- No maneja bien datos de alta dimensionalidad\n",
    "\n",
    "### Resultados Obtenidos:\n",
    "- **Accuracy en test:** {acc_test:.4f}\n",
    "- **F1-Score:** {f1:.4f}\n",
    "- **Validaci√≥n cruzada:** {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\n",
    "\n",
    "### Recomendaciones:\n",
    "1. **Optimizaci√≥n de hiperpar√°metros:** Usar GridSearchCV para encontrar el mejor valor de k\n",
    "2. **Selecci√≥n de features:** Considerar reducci√≥n de dimensionalidad si es necesario\n",
    "3. **Balanceo de clases:** Si las clases est√°n desbalanceadas, considerar t√©cnicas de oversampling/undersampling\n",
    "4. **Comparaci√≥n con otros modelos:** Evaluar contra otros algoritmos de clasificaci√≥n\n",
    "\n",
    "---\n",
    "\n",
    "**üìä Modelo KNN completado exitosamente** ‚úÖ\n",
    "\n",
    "*Los datos est√°n listos para ser utilizados en la aplicaci√≥n o para comparaciones con otros modelos.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}