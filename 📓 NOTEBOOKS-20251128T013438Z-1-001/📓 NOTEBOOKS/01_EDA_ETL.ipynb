{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä An√°lisis Exploratorio de Datos (EDA) y Transformaci√≥n (ETL)\n",
    "## Proyecto: An√°lisis de Exportaciones Colombianas\n",
    "\n",
    "---\n",
    "\n",
    "**Objetivo:** Realizar un an√°lisis exploratorio completo del dataset de exportaciones y preparar los datos para modelos de Machine Learning.\n",
    "\n",
    "**Dataset:** DATAPROYECTO.xlsx - Contiene informaci√≥n detallada sobre exportaciones colombianas\n",
    "\n",
    "**Contenido:**\n",
    "1. Carga y exploraci√≥n inicial\n",
    "2. An√°lisis de valores faltantes\n",
    "3. An√°lisis estad√≠stico descriptivo\n",
    "4. Visualizaciones exploratorias\n",
    "5. Detecci√≥n de outliers\n",
    "6. Transformaci√≥n y limpieza (ETL)\n",
    "7. Feature Engineering\n",
    "8. Exportaci√≥n de datos limpios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importaci√≥n de Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librer√≠as para manipulaci√≥n de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Librer√≠as para visualizaci√≥n\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuraci√≥n de estilo\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Configuraci√≥n de pandas para mejor visualizaci√≥n\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "# Librer√≠as para an√°lisis estad√≠stico\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Ignorar warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset desde Excel\n",
    "df = pd.read_excel('DATAPROYECTO.xlsx', sheet_name='Detalle')\n",
    "\n",
    "# Crear una copia para trabajo\n",
    "df_original = df.copy()\n",
    "\n",
    "print(f\"üì¶ Dataset cargado exitosamente\")\n",
    "print(f\"   Dimensiones: {df.shape[0]:,} filas √ó {df.shape[1]} columnas\")\n",
    "print(f\"   Tama√±o en memoria: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploraci√≥n Inicial del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vista general de las primeras filas\n",
    "print(\"üìã PRIMERAS 10 FILAS DEL DATASET\")\n",
    "print(\"=\"*100)\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n general del dataset\n",
    "print(\"‚ÑπÔ∏è INFORMACI√ìN GENERAL DEL DATASET\")\n",
    "print(\"=\"*100)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombres de las columnas\n",
    "print(\"üìù COLUMNAS DEL DATASET:\")\n",
    "print(\"=\"*100)\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tipos de datos por columna\n",
    "print(\"üî¢ TIPOS DE DATOS:\")\n",
    "print(\"=\"*100)\n",
    "tipos = df.dtypes.value_counts()\n",
    "print(tipos)\n",
    "print(f\"\\nüìä Resumen:\")\n",
    "print(f\"   ‚Ä¢ Num√©ricas (int64): {(df.dtypes == 'int64').sum()}\")\n",
    "print(f\"   ‚Ä¢ Num√©ricas (float64): {(df.dtypes == 'float64').sum()}\")\n",
    "print(f\"   ‚Ä¢ Categ√≥ricas (object): {(df.dtypes == 'object').sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. An√°lisis de Valores Faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular valores nulos\n",
    "missing_data = pd.DataFrame({\n",
    "    'Columna': df.columns,\n",
    "    'Valores_Nulos': df.isnull().sum(),\n",
    "    'Porcentaje_%': (df.isnull().sum() / len(df)) * 100\n",
    "}).sort_values('Valores_Nulos', ascending=False)\n",
    "\n",
    "missing_data = missing_data[missing_data['Valores_Nulos'] > 0]\n",
    "\n",
    "print(\"‚ùå AN√ÅLISIS DE VALORES FALTANTES:\")\n",
    "print(\"=\"*100)\n",
    "if len(missing_data) > 0:\n",
    "    display(missing_data)\n",
    "    print(f\"\\n‚ö†Ô∏è Total de columnas con valores faltantes: {len(missing_data)}\")\n",
    "else:\n",
    "    print(\"‚úÖ No hay valores faltantes en el dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n de valores faltantes\n",
    "if len(missing_data) > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(missing_data['Columna'], missing_data['Porcentaje_%'], color='coral')\n",
    "    plt.xlabel('Porcentaje de Valores Faltantes (%)', fontsize=12)\n",
    "    plt.ylabel('Columnas', fontsize=12)\n",
    "    plt.title('Valores Faltantes por Columna', fontsize=14, fontweight='bold')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    for i, v in enumerate(missing_data['Porcentaje_%']):\n",
    "        plt.text(v + 0.5, i, f'{v:.2f}%', va='center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Estad√≠sticas Descriptivas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad√≠sticas para variables num√©ricas\n",
    "print(\"üìà ESTAD√çSTICAS DESCRIPTIVAS - VARIABLES NUM√âRICAS:\")\n",
    "print(\"=\"*100)\n",
    "display(df.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad√≠sticas para variables categ√≥ricas\n",
    "print(\"üìä ESTAD√çSTICAS DESCRIPTIVAS - VARIABLES CATEG√ìRICAS:\")\n",
    "print(\"=\"*100)\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "cat_stats = pd.DataFrame({\n",
    "    'Columna': categorical_cols,\n",
    "    'Valores_√önicos': [df[col].nunique() for col in categorical_cols],\n",
    "    'Valor_M√°s_Frecuente': [df[col].mode()[0] if len(df[col].mode()) > 0 else None for col in categorical_cols],\n",
    "    'Frecuencia_Max': [df[col].value_counts().iloc[0] for col in categorical_cols]\n",
    "})\n",
    "display(cat_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. An√°lisis de Distribuciones - Variables Num√©ricas Clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar las variables num√©ricas m√°s importantes para visualizaci√≥n\n",
    "numerical_cols_key = ['Valor FOB (USD)', 'Peso en kilos netos', 'Peso en kilos brutos', \n",
    "                      'Cantidad(es)', 'Precio Unitario FOB (USD) Peso Neto']\n",
    "\n",
    "# Crear visualizaciones de distribuci√≥n\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_cols_key):\n",
    "    # Histograma con KDE\n",
    "    sns.histplot(df[col].dropna(), kde=True, ax=axes[idx], color='steelblue')\n",
    "    axes[idx].set_title(f'Distribuci√≥n: {col}', fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Frecuencia')\n",
    "    \n",
    "    # A√±adir estad√≠sticas en el gr√°fico\n",
    "    mean_val = df[col].mean()\n",
    "    median_val = df[col].median()\n",
    "    axes[idx].axvline(mean_val, color='red', linestyle='--', label=f'Media: {mean_val:.2f}')\n",
    "    axes[idx].axvline(median_val, color='green', linestyle='--', label=f'Mediana: {median_val:.2f}')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Las distribuciones muestran el comportamiento de las variables num√©ricas principales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots para detectar outliers\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_cols_key):\n",
    "    sns.boxplot(y=df[col].dropna(), ax=axes[idx], color='lightcoral')\n",
    "    axes[idx].set_title(f'Box Plot: {col}', fontweight='bold')\n",
    "    axes[idx].set_ylabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üì¶ Los box plots permiten identificar valores at√≠picos en cada variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. An√°lisis de Variables Categ√≥ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 pa√≠ses de destino\n",
    "print(\"üåç TOP 10 PA√çSES DE DESTINO:\")\n",
    "print(\"=\"*100)\n",
    "top_paises = df['Pa√≠s de Destino'].value_counts().head(10)\n",
    "print(top_paises)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_paises.plot(kind='barh', color='skyblue')\n",
    "plt.title('Top 10 Pa√≠ses de Destino', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('N√∫mero de Exportaciones')\n",
    "plt.ylabel('Pa√≠s')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuci√≥n por continente\n",
    "print(\"üåé DISTRIBUCI√ìN POR CONTINENTE:\")\n",
    "print(\"=\"*100)\n",
    "continente_dist = df['Continente Destino'].value_counts()\n",
    "print(continente_dist)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.pie(continente_dist.values, labels=continente_dist.index, autopct='%1.1f%%', \n",
    "        startangle=90, colors=sns.color_palette('pastel'))\n",
    "plt.title('Distribuci√≥n de Exportaciones por Continente', fontsize=14, fontweight='bold')\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 departamentos de origen\n",
    "print(\"üìç TOP 10 DEPARTAMENTOS DE ORIGEN:\")\n",
    "print(\"=\"*100)\n",
    "top_deptos = df['Departamento Origen'].value_counts().head(10)\n",
    "print(top_deptos)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_deptos.plot(kind='bar', color='lightgreen')\n",
    "plt.title('Top 10 Departamentos de Origen', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Departamento')\n",
    "plt.ylabel('N√∫mero de Exportaciones')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de v√≠as de transporte\n",
    "print(\"üö¢ V√çAS DE TRANSPORTE:\")\n",
    "print(\"=\"*100)\n",
    "transporte_dist = df['V√≠a de transporte'].value_counts()\n",
    "print(transporte_dist)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "transporte_dist.plot(kind='bar', color='coral')\n",
    "plt.title('Distribuci√≥n por V√≠a de Transporte', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('V√≠a de Transporte')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. An√°lisis de Correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlaci√≥n para variables num√©ricas\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Calcular matriz de correlaci√≥n\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "\n",
    "# Visualizar matriz de correlaci√≥n\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Matriz de Correlaci√≥n - Variables Num√©ricas', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîó CORRELACIONES M√ÅS FUERTES (|r| > 0.7):\")\n",
    "print(\"=\"*100)\n",
    "# Encontrar correlaciones fuertes\n",
    "high_corr = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "            high_corr.append((\n",
    "                correlation_matrix.columns[i],\n",
    "                correlation_matrix.columns[j],\n",
    "                correlation_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "if high_corr:\n",
    "    for var1, var2, corr in high_corr:\n",
    "        print(f\"   ‚Ä¢ {var1} ‚Üî {var2}: r = {corr:.3f}\")\n",
    "else:\n",
    "    print(\"   No se encontraron correlaciones muy fuertes (|r| > 0.7)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Detecci√≥n de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para detectar outliers usando IQR\n",
    "def detect_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Analizar outliers en variables clave\n",
    "print(\"üéØ AN√ÅLISIS DE OUTLIERS (M√©todo IQR):\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "outlier_summary = []\n",
    "for col in numerical_cols_key:\n",
    "    outliers, lower, upper = detect_outliers_iqr(df, col)\n",
    "    pct_outliers = (len(outliers) / len(df)) * 100\n",
    "    outlier_summary.append({\n",
    "        'Variable': col,\n",
    "        'Outliers': len(outliers),\n",
    "        'Porcentaje_%': pct_outliers,\n",
    "        'L√≠mite_Inferior': lower,\n",
    "        'L√≠mite_Superior': upper\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "display(outlier_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ETL - Transformaci√≥n y Limpieza de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear copia para transformaci√≥n\n",
    "df_clean = df.copy()\n",
    "\n",
    "print(\"üîß INICIANDO PROCESO ETL...\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# 1. Manejo de valores faltantes\n",
    "print(\"\\n1Ô∏è‚É£ Tratamiento de valores faltantes:\")\n",
    "\n",
    "# Para columnas num√©ricas: imputar con la mediana\n",
    "numeric_cols_with_nulls = df_clean.select_dtypes(include=[np.number]).columns[df_clean.select_dtypes(include=[np.number]).isnull().any()].tolist()\n",
    "for col in numeric_cols_with_nulls:\n",
    "    median_val = df_clean[col].median()\n",
    "    df_clean[col].fillna(median_val, inplace=True)\n",
    "    print(f\"   ‚úì {col}: Imputado con mediana = {median_val:.2f}\")\n",
    "\n",
    "# Para columnas categ√≥ricas: imputar con 'Desconocido'\n",
    "categorical_cols_with_nulls = df_clean.select_dtypes(include=['object']).columns[df_clean.select_dtypes(include=['object']).isnull().any()].tolist()\n",
    "for col in categorical_cols_with_nulls:\n",
    "    df_clean[col].fillna('Desconocido', inplace=True)\n",
    "    print(f\"   ‚úì {col}: Imputado con 'Desconocido'\")\n",
    "\n",
    "print(f\"\\n   Total valores nulos restantes: {df_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Eliminaci√≥n de duplicados\n",
    "print(\"\\n2Ô∏è‚É£ Eliminaci√≥n de duplicados:\")\n",
    "duplicates_before = df_clean.duplicated().sum()\n",
    "df_clean.drop_duplicates(inplace=True)\n",
    "duplicates_after = df_clean.duplicated().sum()\n",
    "print(f\"   ‚úì Duplicados eliminados: {duplicates_before}\")\n",
    "print(f\"   ‚úì Filas restantes: {len(df_clean):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Normalizaci√≥n de texto en columnas categ√≥ricas\n",
    "print(\"\\n3Ô∏è‚É£ Normalizaci√≥n de texto:\")\n",
    "text_columns = ['Pa√≠s de Destino', 'Departamento Origen', 'Continente Destino', \n",
    "                'V√≠a de transporte', 'Raz√≥n social actual Exportador']\n",
    "\n",
    "for col in text_columns:\n",
    "    if col in df_clean.columns:\n",
    "        # Convertir a may√∫sculas y eliminar espacios extras\n",
    "        df_clean[col] = df_clean[col].str.upper().str.strip()\n",
    "        print(f\"   ‚úì {col}: Normalizado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Creaci√≥n de variables derivadas\n",
    "print(\"\\n4Ô∏è‚É£ Feature Engineering - Creaci√≥n de nuevas variables:\")\n",
    "\n",
    "# Ratio Peso Bruto/Neto\n",
    "df_clean['Ratio_Peso_Bruto_Neto'] = df_clean['Peso en kilos brutos'] / (df_clean['Peso en kilos netos'] + 1e-10)\n",
    "print(\"   ‚úì Ratio_Peso_Bruto_Neto: Creada\")\n",
    "\n",
    "# Valor por kilogramo\n",
    "df_clean['Valor_Por_Kg'] = df_clean['Valor FOB (USD)'] / (df_clean['Peso en kilos netos'] + 1e-10)\n",
    "print(\"   ‚úì Valor_Por_Kg: Creada\")\n",
    "\n",
    "# Clasificaci√≥n de valor de exportaci√≥n\n",
    "def clasificar_valor(valor):\n",
    "    if valor < 1000:\n",
    "        return 'Bajo'\n",
    "    elif valor < 10000:\n",
    "        return 'Medio'\n",
    "    elif valor < 100000:\n",
    "        return 'Alto'\n",
    "    else:\n",
    "        return 'Muy Alto'\n",
    "\n",
    "df_clean['Categoria_Valor'] = df_clean['Valor FOB (USD)'].apply(clasificar_valor)\n",
    "print(\"   ‚úì Categoria_Valor: Creada (Bajo/Medio/Alto/Muy Alto)\")\n",
    "\n",
    "# Eficiencia de transporte (Valor por art√≠culo)\n",
    "df_clean['Valor_Por_Articulo'] = df_clean['Valor FOB (USD)'] / (df_clean['N√∫mero de art√≠culos'] + 1e-10)\n",
    "print(\"   ‚úì Valor_Por_Articulo: Creada\")\n",
    "\n",
    "# Peso promedio por art√≠culo\n",
    "df_clean['Peso_Promedio_Articulo'] = df_clean['Peso en kilos netos'] / (df_clean['N√∫mero de art√≠culos'] + 1e-10)\n",
    "print(\"   ‚úì Peso_Promedio_Articulo: Creada\")\n",
    "\n",
    "print(f\"\\n   üìä Total de columnas despu√©s de Feature Engineering: {len(df_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Manejo de outliers extremos (opcional - usar con cuidado)\n",
    "print(\"\\n5Ô∏è‚É£ Manejo de outliers extremos:\")\n",
    "print(\"   ‚ÑπÔ∏è Se aplicar√° winsorizaci√≥n para valores extremos (percentil 1% y 99%)\")\n",
    "\n",
    "# Aplicar winsorizaci√≥n a las variables num√©ricas clave\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "cols_to_winsorize = ['Valor FOB (USD)', 'Peso en kilos netos', 'Cantidad(es)', 'Valor_Por_Kg']\n",
    "\n",
    "for col in cols_to_winsorize:\n",
    "    if col in df_clean.columns:\n",
    "        # Winsorizar entre el 1% y 99%\n",
    "        df_clean[col + '_winsorized'] = winsorize(df_clean[col], limits=[0.01, 0.01])\n",
    "        print(f\"   ‚úì {col}: Winsorizado (1%-99%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Codificaci√≥n de Variables Categ√≥ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî¢ CODIFICACI√ìN DE VARIABLES CATEG√ìRICAS:\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Crear copia para codificaci√≥n\n",
    "df_encoded = df_clean.copy()\n",
    "\n",
    "# Seleccionar columnas categ√≥ricas principales para codificar\n",
    "categorical_to_encode = ['Pa√≠s de Destino', 'Continente Destino', 'Departamento Origen',\n",
    "                         'V√≠a de transporte', 'Moneda de negociaci√≥n', 'Forma de pago',\n",
    "                         'Aduana De Embarque', 'Categoria_Valor']\n",
    "\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_to_encode:\n",
    "    if col in df_encoded.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_encoded[col + '_encoded'] = le.fit_transform(df_encoded[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"   ‚úì {col}: Codificado ({df_encoded[col].nunique()} categor√≠as)\")\n",
    "\n",
    "print(f\"\\n   Total de variables codificadas: {len(label_encoders)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Escalamiento de Variables Num√©ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìè ESCALAMIENTO DE VARIABLES NUM√âRICAS:\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Variables num√©ricas a escalar\n",
    "numeric_cols_to_scale = ['Valor FOB (USD)', 'Peso en kilos netos', 'Peso en kilos brutos',\n",
    "                         'Cantidad(es)', 'Precio Unitario FOB (USD) Peso Neto',\n",
    "                         'Ratio_Peso_Bruto_Neto', 'Valor_Por_Kg', 'Valor_Por_Articulo']\n",
    "\n",
    "# StandardScaler (Z-score normalization)\n",
    "scaler_standard = StandardScaler()\n",
    "for col in numeric_cols_to_scale:\n",
    "    if col in df_encoded.columns:\n",
    "        df_encoded[col + '_scaled'] = scaler_standard.fit_transform(df_encoded[[col]])\n",
    "\n",
    "print(\"   ‚úì StandardScaler aplicado (media=0, std=1)\")\n",
    "\n",
    "# MinMaxScaler (normalizaci√≥n 0-1)\n",
    "scaler_minmax = MinMaxScaler()\n",
    "for col in numeric_cols_to_scale:\n",
    "    if col in df_encoded.columns:\n",
    "        df_encoded[col + '_normalized'] = scaler_minmax.fit_transform(df_encoded[[col]])\n",
    "\n",
    "print(\"   ‚úì MinMaxScaler aplicado (rango 0-1)\")\n",
    "print(f\"\\n   Total de columnas despu√©s del escalamiento: {len(df_encoded.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Validaci√≥n de Datos Limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ VALIDACI√ìN DE DATOS LIMPIOS:\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\nüìä Dimensiones finales:\")\n",
    "print(f\"   ‚Ä¢ Filas: {df_encoded.shape[0]:,}\")\n",
    "print(f\"   ‚Ä¢ Columnas: {df_encoded.shape[1]}\")\n",
    "\n",
    "print(f\"\\n‚ùå Valores nulos:\")\n",
    "nulls = df_encoded.isnull().sum().sum()\n",
    "print(f\"   ‚Ä¢ Total: {nulls}\")\n",
    "\n",
    "print(f\"\\nüî¢ Tipos de datos:\")\n",
    "print(f\"   ‚Ä¢ Num√©ricas: {len(df_encoded.select_dtypes(include=[np.number]).columns)}\")\n",
    "print(f\"   ‚Ä¢ Categ√≥ricas: {len(df_encoded.select_dtypes(include=['object']).columns)}\")\n",
    "\n",
    "print(f\"\\nüíæ Tama√±o en memoria: {df_encoded.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"‚úÖ PROCESO ETL COMPLETADO EXITOSAMENTE\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Exportaci√≥n de Datos Procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ EXPORTANDO DATASETS PROCESADOS...\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Guardar diferentes versiones del dataset\n",
    "\n",
    "# 1. Dataset limpio sin codificar (para an√°lisis)\n",
    "df_clean.to_csv('data_clean.csv', index=False)\n",
    "print(\"   ‚úì data_clean.csv guardado\")\n",
    "\n",
    "# 2. Dataset codificado y escalado (para ML)\n",
    "df_encoded.to_csv('data_processed.csv', index=False)\n",
    "print(\"   ‚úì data_processed.csv guardado\")\n",
    "\n",
    "# 3. Dataset solo con variables num√©ricas (para clustering)\n",
    "df_numeric = df_encoded.select_dtypes(include=[np.number])\n",
    "df_numeric.to_csv('data_numeric.csv', index=False)\n",
    "print(\"   ‚úì data_numeric.csv guardado\")\n",
    "\n",
    "# 4. Guardar tambi√©n en formato pickle para preservar tipos de datos\n",
    "import pickle\n",
    "\n",
    "with open('data_processed.pkl', 'wb') as f:\n",
    "    pickle.dump(df_encoded, f)\n",
    "print(\"   ‚úì data_processed.pkl guardado\")\n",
    "\n",
    "# 5. Guardar los encoders y scalers\n",
    "with open('label_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "print(\"   ‚úì label_encoders.pkl guardado\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"‚úÖ TODOS LOS ARCHIVOS GUARDADOS EXITOSAMENTE\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Resumen Final del EDA y ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìã RESUMEN EJECUTIVO - EDA Y ETL\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\nüîç DATOS ORIGINALES:\")\n",
    "print(f\"   ‚Ä¢ Registros: {df_original.shape[0]:,}\")\n",
    "print(f\"   ‚Ä¢ Variables: {df_original.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Valores nulos: {df_original.isnull().sum().sum():,}\")\n",
    "\n",
    "print(\"\\nüõ†Ô∏è TRANSFORMACIONES APLICADAS:\")\n",
    "print(\"   1. ‚úì Imputaci√≥n de valores faltantes\")\n",
    "print(\"   2. ‚úì Eliminaci√≥n de duplicados\")\n",
    "print(\"   3. ‚úì Normalizaci√≥n de texto\")\n",
    "print(\"   4. ‚úì Feature Engineering (5 nuevas variables)\")\n",
    "print(\"   5. ‚úì Tratamiento de outliers (winsorizaci√≥n)\")\n",
    "print(\"   6. ‚úì Codificaci√≥n de variables categ√≥ricas\")\n",
    "print(\"   7. ‚úì Escalamiento de variables num√©ricas\")\n",
    "\n",
    "print(\"\\nüìä DATOS PROCESADOS:\")\n",
    "print(f\"   ‚Ä¢ Registros finales: {df_encoded.shape[0]:,}\")\n",
    "print(f\"   ‚Ä¢ Variables finales: {df_encoded.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Nuevas variables creadas: {df_encoded.shape[1] - df_original.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Valores nulos: {df_encoded.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\nüíæ ARCHIVOS GENERADOS:\")\n",
    "print(\"   ‚Ä¢ data_clean.csv - Dataset limpio sin codificar\")\n",
    "print(\"   ‚Ä¢ data_processed.csv - Dataset completo procesado\")\n",
    "print(\"   ‚Ä¢ data_numeric.csv - Solo variables num√©ricas\")\n",
    "print(\"   ‚Ä¢ data_processed.pkl - Dataset en formato pickle\")\n",
    "print(\"   ‚Ä¢ label_encoders.pkl - Encoders guardados\")\n",
    "\n",
    "print(\"\\nüéØ INSIGHTS PRINCIPALES:\")\n",
    "print(f\"   ‚Ä¢ Principal pa√≠s destino: {df_clean['Pa√≠s de Destino'].mode()[0]}\")\n",
    "print(f\"   ‚Ä¢ Principal continente: {df_clean['Continente Destino'].mode()[0]}\")\n",
    "print(f\"   ‚Ä¢ Departamento con m√°s exportaciones: {df_clean['Departamento Origen'].mode()[0]}\")\n",
    "print(f\"   ‚Ä¢ V√≠a de transporte m√°s usada: {df_clean['V√≠a de transporte'].mode()[0]}\")\n",
    "print(f\"   ‚Ä¢ Valor FOB promedio: ${df_clean['Valor FOB (USD)'].mean():,.2f} USD\")\n",
    "print(f\"   ‚Ä¢ Valor FOB mediano: ${df_clean['Valor FOB (USD)'].median():,.2f} USD\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üéâ AN√ÅLISIS EXPLORATORIO Y ETL FINALIZADOS\")\n",
    "print(\"üìå Los datos est√°n listos para ser utilizados en modelos de Machine Learning\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Conclusiones y Pr√≥ximos Pasos\n",
    "\n",
    "### Hallazgos Principales del EDA:\n",
    "\n",
    "1. **Calidad de Datos**: El dataset presentaba valores faltantes moderados que fueron tratados adecuadamente\n",
    "2. **Distribuciones**: Las variables num√©ricas muestran distribuciones asim√©tricas con presencia de outliers\n",
    "3. **Correlaciones**: Se identificaron correlaciones esperadas entre peso bruto/neto y valores FOB\n",
    "4. **Categor√≠as Dominantes**: Concentraci√≥n en ciertos pa√≠ses, departamentos y v√≠as de transporte\n",
    "\n",
    "### Transformaciones ETL Realizadas:\n",
    "\n",
    "- ‚úÖ Limpieza y normalizaci√≥n de datos\n",
    "- ‚úÖ Creaci√≥n de variables derivadas relevantes\n",
    "- ‚úÖ Codificaci√≥n de variables categ√≥ricas\n",
    "- ‚úÖ Escalamiento de variables num√©ricas\n",
    "- ‚úÖ Preparaci√≥n de m√∫ltiples versiones del dataset\n",
    "\n",
    "### Datasets Disponibles:\n",
    "\n",
    "1. **data_clean.csv**: Para an√°lisis exploratorios adicionales\n",
    "2. **data_processed.csv**: Para modelos supervisados de ML\n",
    "3. **data_numeric.csv**: Para modelos de clustering\n",
    "4. **data_processed.pkl**: Para mantener tipos de datos Python\n",
    "\n",
    "### Pr√≥ximos Pasos:\n",
    "\n",
    "1. **Regresi√≥n Lineal**: Predecir Valor FOB en funci√≥n de variables continuas\n",
    "2. **Regresi√≥n Log√≠stica**: Clasificar categor√≠as de valor de exportaci√≥n\n",
    "3. **KNN**: Clasificaci√≥n basada en similitud\n",
    "4. **SVM**: Clasificaci√≥n con m√°rgenes √≥ptimos\n",
    "5. **√Årboles de Decisi√≥n**: Modelo interpretable de clasificaci√≥n\n",
    "6. **Naive Bayes**: Clasificaci√≥n probabil√≠stica\n",
    "7. **K-Means**: Segmentaci√≥n de exportaciones similares\n",
    "\n",
    "---\n",
    "\n",
    "**Autor**: Proyecto Machine Learning - An√°lisis de Exportaciones\n",
    "\n",
    "**Fecha**: 2025\n",
    "\n",
    "**Versi√≥n**: 1.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}